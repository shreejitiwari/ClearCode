{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d92c720",
   "metadata": {},
   "source": [
    "# Churn Prediction using Artificial Neural Networks (ANN)\n",
    "\n",
    "Churn refers to the phenomenon where customers or users of a product or service discontinue their usage or subscription. It is a critical metric for businesses, as it directly impacts revenue and customer retention.\n",
    "\n",
    "Artificial Neural Networks (ANN) are a type of machine learning model inspired by the structure and function of the human brain. ANN can be used to predict churn by analyzing patterns and relationships in large datasets.\n",
    "\n",
    "## Steps for Churn Prediction using ANN:\n",
    "\n",
    "1. **Data Preprocessing**: This step involves cleaning and preparing the dataset for analysis. It includes handling missing values, encoding categorical variables, and scaling numerical features.\n",
    "\n",
    "2. **Building the ANN Model**: In this step, we define the architecture of the ANN model. It typically consists of an input layer, one or more hidden layers, and an output layer. Each layer contains multiple neurons that perform computations.\n",
    "\n",
    "3. **Training the ANN Model**: The model is trained using a training dataset, where the input features are used to predict the churn outcome. During training, the model adjusts its weights and biases to minimize the prediction error.\n",
    "\n",
    "4. **Evaluating the Model**: After training, the model is evaluated using a separate validation or test dataset. Various evaluation metrics such as accuracy, precision, recall, and F1 score can be used to assess the performance of the model.\n",
    "\n",
    "5. **Making Predictions**: Once the model is trained and evaluated, it can be used to make predictions on new, unseen data. The model takes the input features and predicts the likelihood of churn for each customer.\n",
    "\n",
    "6. **Taking Action**: Based on the predictions, businesses can take proactive measures to retain customers who are at high risk of churn. This can include targeted marketing campaigns, personalized offers, or improved customer service.\n",
    "\n",
    "Churn prediction using ANN is a powerful technique that can help businesses identify and mitigate customer churn. By understanding the factors that contribute to churn and leveraging the predictive capabilities of ANN, businesses can take proactive steps to retain valuable customers and improve overall customer satisfaction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0eb4c7",
   "metadata": {},
   "source": [
    "## Part 1 - Data Preprocessing\n",
    "\n",
    "In this code cell, we are performing data preprocessing steps on a dataset for churn prediction using artificial neural networks (ANN).\n",
    "\n",
    "### Importing the libraries\n",
    "\n",
    "We start by importing the necessary libraries for data preprocessing and analysis: `numpy`, `matplotlib.pyplot`, and `pandas`.\n",
    "\n",
    "### Importing the dataset\n",
    "\n",
    "Next, we import the dataset using the `pd.read_csv()` function from the `pandas` library. The dataset is stored in a CSV file named \"Churn_Modelling.csv\". We assign the features to the variable `X` and the target variable to the variable `y`.\n",
    "\n",
    "### Encoding categorical data\n",
    "\n",
    "To handle categorical data in the dataset, we use the `LabelEncoder` and `OneHotEncoder` classes from the `sklearn.preprocessing` module. We create a `ColumnTransformer` object named `ct` to apply the encoders to specific columns. The `OneHotEncoder` is applied to the column with index 1, representing the \"Geography\" feature. The remaining columns are passed through without any transformation. The transformed data is stored in the variable `X`.\n",
    "\n",
    "### Splitting the dataset into the Training set and Test set\n",
    "\n",
    "We split the dataset into training and test sets using the `train_test_split()` function from the `sklearn.model_selection` module. The `X` and `y` variables are split into `X_train`, `X_test`, `y_train`, and `y_test` with a test size of 0.2 (20% of the data) and a random state of 0.\n",
    "\n",
    "### Feature Scaling\n",
    "\n",
    "To ensure that all features are on the same scale, we perform feature scaling using the `StandardScaler` class from the `sklearn.preprocessing` module. We fit the scaler on the training set (`X_train`) and transform both the training and test sets (`X_train` and `X_test`) using the `fit_transform()` and `transform()` methods, respectively. The scaled test set is stored in the variable `X_test`.\n",
    "\n",
    "The code cell outputs the transformed `X` values and the scaled `X_test` values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d27ad20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' ... 1 1 101348.88]\n",
      " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
      " [502 'France' 'Female' ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 'Female' ... 0 1 42085.58]\n",
      " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
      " [792 'France' 'Female' ... 1 0 38190.78]]\n",
      "[1 0 1 ... 1 1 0]\n",
      "[[0.0 0.0 619 ... 1 1 101348.88]\n",
      " [0.0 1.0 608 ... 0 1 112542.58]\n",
      " [0.0 0.0 502 ... 1 0 113931.57]\n",
      " ...\n",
      " [0.0 0.0 709 ... 0 1 42085.58]\n",
      " [1.0 0.0 772 ... 1 0 92888.52]\n",
      " [0.0 0.0 792 ... 1 0 38190.78]]\n",
      "[[ 1.75486502 -0.57369368 -0.55204276 ...  0.64259497  0.9687384\n",
      "   1.61085707]\n",
      " [-0.5698444  -0.57369368 -1.31490297 ...  0.64259497 -1.03227043\n",
      "   0.49587037]\n",
      " [-0.5698444   1.74309049  0.57162971 ...  0.64259497  0.9687384\n",
      "  -0.42478674]\n",
      " ...\n",
      " [-0.5698444   1.74309049 -0.74791227 ...  0.64259497 -1.03227043\n",
      "   0.71888467]\n",
      " [ 1.75486502 -0.57369368 -0.00566991 ...  0.64259497  0.9687384\n",
      "  -1.54507805]\n",
      " [ 1.75486502 -0.57369368 -0.79945688 ...  0.64259497 -1.03227043\n",
      "   1.61255917]]\n",
      "Epoch 1/100\n",
      "800/800 [==============================] - 19s 914us/step - loss: 0.5554 - accuracy: 0.8001\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 1s 974us/step - loss: 0.4230 - accuracy: 0.7933\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 1s 934us/step - loss: 0.4217 - accuracy: 0.7913\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 1s 896us/step - loss: 0.4229 - accuracy: 0.8201\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 1s 897us/step - loss: 0.4203 - accuracy: 0.8219\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 1s 849us/step - loss: 0.4282 - accuracy: 0.8233\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4056 - accuracy: 0.8352\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4134 - accuracy: 0.8334\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4122 - accuracy: 0.8307\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4195 - accuracy: 0.8271\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3990 - accuracy: 0.8373\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 1s 858us/step - loss: 0.4071 - accuracy: 0.8343\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 1s 959us/step - loss: 0.4093 - accuracy: 0.8328\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 1s 909us/step - loss: 0.4131 - accuracy: 0.8288\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 1s 918us/step - loss: 0.4140 - accuracy: 0.8317\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 1s 938us/step - loss: 0.4117 - accuracy: 0.8365\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 1s 900us/step - loss: 0.4042 - accuracy: 0.8327\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 1s 902us/step - loss: 0.4034 - accuracy: 0.8338\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 1s 917us/step - loss: 0.3988 - accuracy: 0.8389\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 1s 914us/step - loss: 0.4008 - accuracy: 0.8355\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 1s 960us/step - loss: 0.4047 - accuracy: 0.8333\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 1s 929us/step - loss: 0.4006 - accuracy: 0.8372\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 1s 894us/step - loss: 0.4012 - accuracy: 0.8370\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 1s 931us/step - loss: 0.3870 - accuracy: 0.8436\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 1s 950us/step - loss: 0.4002 - accuracy: 0.8353\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 1s 925us/step - loss: 0.4100 - accuracy: 0.8250\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 1s 892us/step - loss: 0.4203 - accuracy: 0.8215\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 1s 814us/step - loss: 0.4031 - accuracy: 0.8332\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 1s 923us/step - loss: 0.3988 - accuracy: 0.8378\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 1s 964us/step - loss: 0.3972 - accuracy: 0.8418\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 1s 981us/step - loss: 0.4014 - accuracy: 0.8370\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4036 - accuracy: 0.8310\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 1s 959us/step - loss: 0.4011 - accuracy: 0.8328\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 1s 967us/step - loss: 0.4021 - accuracy: 0.8354\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 1s 942us/step - loss: 0.3930 - accuracy: 0.8388\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 1s 943us/step - loss: 0.3989 - accuracy: 0.8356\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 1s 911us/step - loss: 0.3980 - accuracy: 0.8401\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 1s 978us/step - loss: 0.3974 - accuracy: 0.8345\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4062 - accuracy: 0.8295\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 1s 949us/step - loss: 0.4044 - accuracy: 0.8334\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4132 - accuracy: 0.8334\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 1s 977us/step - loss: 0.3960 - accuracy: 0.8358\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 1s 883us/step - loss: 0.3964 - accuracy: 0.8433\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3889 - accuracy: 0.8393\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 1s 967us/step - loss: 0.3965 - accuracy: 0.8378\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 1s 981us/step - loss: 0.3966 - accuracy: 0.8364\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 1s 929us/step - loss: 0.3933 - accuracy: 0.8407\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 1s 932us/step - loss: 0.4005 - accuracy: 0.8328\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 1s 944us/step - loss: 0.4052 - accuracy: 0.8315\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 1s 933us/step - loss: 0.3964 - accuracy: 0.8347\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 1s 901us/step - loss: 0.3967 - accuracy: 0.8396\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 1s 935us/step - loss: 0.3961 - accuracy: 0.8405\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 1s 958us/step - loss: 0.4098 - accuracy: 0.8307\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 1s 945us/step - loss: 0.3940 - accuracy: 0.8423\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 1s 990us/step - loss: 0.4066 - accuracy: 0.8336\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 1s 860us/step - loss: 0.3953 - accuracy: 0.8356\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 1s 893us/step - loss: 0.3966 - accuracy: 0.8391\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 1s 907us/step - loss: 0.3973 - accuracy: 0.8392\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4097 - accuracy: 0.8297\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3985 - accuracy: 0.8394\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 1s 918us/step - loss: 0.3938 - accuracy: 0.8378\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4028 - accuracy: 0.8361\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3958 - accuracy: 0.8338\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4139 - accuracy: 0.8294\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4035 - accuracy: 0.8315\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 1s 995us/step - loss: 0.3941 - accuracy: 0.8375\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 1s 980us/step - loss: 0.3971 - accuracy: 0.8354\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3920 - accuracy: 0.8377\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3982 - accuracy: 0.8395\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3975 - accuracy: 0.8362\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4008 - accuracy: 0.8302\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3911 - accuracy: 0.8394: 0s - loss:\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3922 - accuracy: 0.8403\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4071 - accuracy: 0.8328\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4017 - accuracy: 0.8284\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3970 - accuracy: 0.8400\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3961 - accuracy: 0.8341\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3965 - accuracy: 0.8327\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3920 - accuracy: 0.8393\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3874 - accuracy: 0.8408\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4058 - accuracy: 0.8325\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4147 - accuracy: 0.8254\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4055 - accuracy: 0.8338\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3922 - accuracy: 0.8409\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3928 - accuracy: 0.8373\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3943 - accuracy: 0.8394\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3986 - accuracy: 0.8357\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4041 - accuracy: 0.8321\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3988 - accuracy: 0.8362\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4067 - accuracy: 0.8346\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3909 - accuracy: 0.8420\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4005 - accuracy: 0.8334\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3965 - accuracy: 0.8382\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3982 - accuracy: 0.8356\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3957 - accuracy: 0.8354\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3838 - accuracy: 0.8379\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 1s 921us/step - loss: 0.3924 - accuracy: 0.8357\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 1s 945us/step - loss: 0.3999 - accuracy: 0.8308\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 1s 897us/step - loss: 0.3881 - accuracy: 0.8421\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 1s 806us/step - loss: 0.4003 - accuracy: 0.8393\n",
      "[[1540   55]\n",
      " [ 253  152]]\n"
     ]
    }
   ],
   "source": [
    "# Part 1 - Data Preprocessing\n",
    "\n",
    "# Importing the libraries\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    columnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [0])],     remainder='passthrough')\n",
    "    X=np.array(columnTransformer.fit_transform(X),dtype=np.str)\n",
    "Since the latest build of sklearn library \n",
    "removed categorical_features parameter \n",
    "for onehotencoder class. \n",
    "It is advised to use ColumnTransformer class for categorical datasets. \n",
    "Refer the sklearn's official documentation for futher clarifications.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv(\"E:\\\\2021\\\\notes 2021\\\\CSA501 Deep learning_amity\\\\data set\\\\Churn_Modelling.csv\")\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "print (X)\n",
    "print (y)\n",
    "\n",
    "\n",
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "ct = ColumnTransformer([(\"Geography\",OneHotEncoder(),[1])], remainder= 'passthrough')\n",
    "ct\n",
    "X = ct.fit_transform(X)\n",
    "labelencoder_X2 = LabelEncoder()\n",
    "X[:, 4] = labelencoder_X2.fit_transform(X[:, 4])\n",
    "X = X[: , 1:]\n",
    "print (X)\n",
    "\n",
    "\"\"\"\n",
    "X = np.array(X, dtype=float)\n",
    "\n",
    "Just adding an extra line to convert it from array of objects.\n",
    "\"\"\"\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "print(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbcdfe1",
   "metadata": {},
   "source": [
    "## Artificial Neural Network (ANN) Documentation\n",
    "\n",
    "This documentation provides an overview of the code that builds an Artificial Neural Network (ANN) for predicting churn. It explains the libraries imported and their usage in building the ANN.\n",
    "\n",
    "### Libraries Used\n",
    "\n",
    "The following libraries are imported in the code:\n",
    "\n",
    "1. `keras`: Keras is a high-level neural networks API written in Python. It provides a user-friendly interface for building and training deep learning models.\n",
    "\n",
    "2. `Sequential`: Sequential is a class from the Keras library that allows us to build a neural network model layer by layer.\n",
    "\n",
    "3. `Dense`: Dense is a class from the Keras library that represents a fully connected layer in a neural network. It is used to add layers to the neural network model.\n",
    "\n",
    "### Code Explanation\n",
    "\n",
    "The code is divided into three parts: data preprocessing, building the ANN, and making predictions.\n",
    "\n",
    "#### Part 1 - Data Preprocessing\n",
    "\n",
    "This part is done in the above code cell.\n",
    "\n",
    "#### Part 2 - Building the ANN\n",
    "\n",
    "The code starts by initializing the ANN using the `Sequential` class. Then, it adds layers to the ANN using the `Dense` class.\n",
    "\n",
    "1. Adding the input layer and the first hidden layer:\n",
    "    - `classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=11))`\n",
    "    - This line adds the first hidden layer to the ANN. It has 6 neurons, uses the 'relu' activation function, and expects an input dimension of 11.\n",
    "\n",
    "2. Adding the second hidden layer:\n",
    "    - `classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))`\n",
    "    - This line adds the second hidden layer to the ANN. It has 6 neurons and uses the 'relu' activation function.\n",
    "\n",
    "3. Adding the output layer:\n",
    "    - `classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))`\n",
    "    - This line adds the output layer to the ANN. It has 1 neuron and uses the 'sigmoid' activation function.\n",
    "\n",
    "4. Compiling the ANN:\n",
    "    - `classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])`\n",
    "    - This line compiles the ANN by specifying the optimizer, loss function, and metrics to be used during training.\n",
    "\n",
    "#### Part 3 - Making Predictions and Evaluating the Model\n",
    "\n",
    "1. Fitting the ANN to the Training set:\n",
    "    - `classifier.fit(X_train, y_train, batch_size=10, epochs=100)`\n",
    "    - This line trains the ANN on the training set. It specifies the batch size and number of epochs for training.\n",
    "\n",
    "2. Predicting the Test set results:\n",
    "    - `y_pred = classifier.predict(X_test)`\n",
    "    - This line predicts the churn values for the test set using the trained ANN.\n",
    "\n",
    "3. Making the Confusion Matrix:\n",
    "    - `from sklearn.metrics import confusion_matrix`\n",
    "    - `cm = confusion_matrix(y_test, y_pred)`\n",
    "    - This code calculates the confusion matrix to evaluate the performance of the model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da13859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 - Now let's make the ANN!\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "#1. model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n",
    "\n",
    "#3It means 8 input parameters,  with 12 neurons in the FIRST hidden layer.\n",
    "2. #In Keras, \"dense\" usually refers to a single layer, whereas \"sequential\" \n",
    "#usually refers to an entire model, not just one layer. ... \n",
    "#Sequential refers to the way you build models in Keras using the sequential api\n",
    "3. #Output_dim is the dimension of the dense embedding. The choice of 128 in classifier.add(Dense(output_dim = 128, activation = 'relu')) is quite arbitrary ,\n",
    "# it just indicate the size of fully connected layer that you prefer. uniform distribution \n",
    "#input dim. Sometimes, though, you just have one dimension â€“ which is the case with one-dimensional / flattened arrays,\n",
    "#4. #Adam optimization is an extension to Stochastic gradient decent and can be used in place\n",
    "# of classical \n",
    "#stochastic gradient descent to update network weights more efficiently.\n",
    "# Part 1 - Data Preprocessing\n",
    "# Part 2 - Now let's make the ANN! # Part 3 - Making the predictions and evaluating the model\n",
    "\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "#classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))\n",
    "#classifier.add(Dense(6, init = 'uniform', activation = 'relu', input_dim = 11))\n",
    "classifier.add(Dense(units =6, kernel_initializer = 'uniform' , activation = 'relu', input_dim =11 ))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "#classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "#classifier.add(Dense(Output_dim = 6 , kernel_initializer = 'uniform' , activation = 'relu'))\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)\n",
    "\n",
    "# Part 3 - Making the predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print (cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "375edd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "?OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3b1d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "?LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a0b1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "?ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc509f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' ... 1 1 101348.88]\n",
      " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
      " [502 'France' 'Female' ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 'Female' ... 0 1 42085.58]\n",
      " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
      " [792 'France' 'Female' ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv(\"E:\\\\2021\\\\notes 2021\\\\CSA501 Deep learning_amity\\\\data set\\\\Churn_Modelling.csv\")\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "print (X)\n",
    "#print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01320033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `iloc` not found.\n"
     ]
    }
   ],
   "source": [
    "?iloc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7123b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "?StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04533acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "?Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19173090",
   "metadata": {},
   "outputs": [],
   "source": [
    "?Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e436052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `classfier.add()` not found.\n"
     ]
    }
   ],
   "source": [
    "?classfier.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "500a7fb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'testCases_v4a'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-45c6344f035d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtestCases_v4a\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdnn_utils_v2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigmoid_backward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelu_backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'testCases_v4a'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1de5d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "?confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "277bf646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv(\"E:\\\\2021\\\\notes 2021\\\\CSA501 Deep learning_amity\\\\data set\\\\Churn_Modelling.csv\")\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "print (X)\n",
    "print (y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65d89804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColumnTransformer(remainder='passthrough',\n",
      "                  transformers=[('Geography', OneHotEncoder(), [1])])\n",
      "[[0.0 0.0 619 ... 1 1 101348.88]\n",
      " [1.0 0.0 608 ... 0 1 112542.58]\n",
      " [0.0 0.0 502 ... 1 0 113931.57]\n",
      " ...\n",
      " [0.0 0.0 709 ... 0 1 42085.58]\n",
      " [0.0 1.0 772 ... 1 0 92888.52]\n",
      " [0.0 0.0 792 ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "ct = ColumnTransformer([(\"Geography\",OneHotEncoder(),[1])], remainder= 'passthrough')\n",
    "ct\n",
    "print(ct)\n",
    "X = ct.fit_transform(X)\n",
    "labelencoder_X2 = LabelEncoder()\n",
    "X[:, 4] = labelencoder_X2.fit_transform(X[:, 4])\n",
    "X = X[: , 1:]\n",
    "print (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a53d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LabelEncoder\n",
    "#OneHotEncoder\n",
    "#ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7b977a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 1. , 0.5, 0.5],\n",
       "       [0.5, 0.5, 0. , 1. ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "ct = ColumnTransformer(\n",
    "     [(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n",
    "      (\"norm2\", Normalizer(norm='l1'), slice(2, 4))])\n",
    "X = np.array([[0., 1., 2., 2.],\n",
    "              [1., 1., 0., 1.]])\n",
    " # Normalizer scales each row of X to unit norm. A separate scaling\n",
    " # is applied for the two first and two last elements of each\n",
    " # row independently.\n",
    "ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b4c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
